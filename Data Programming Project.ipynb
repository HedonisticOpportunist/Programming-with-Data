{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15fdf776",
   "metadata": {},
   "source": [
    "# Software Testing, Automation and the Job Market - a Data Programming Project\n",
    "\n",
    "### Research Questions, Project Limitations and Motivation \n",
    "\n",
    "#### Research Area\n",
    "The goal of this project is to analyse a given dataset from a large job board in regards to software testing roles with the emphasis of the analysis placed on what the most sought for skills are in order to shed some light on the ongoing debate on whether automation testing is going to replace manual testing (e.g __[Can Manual Testing Be Completely Replaced by Automation Testing?](https://blog.qasource.com/resources/can-manual-testing-be-completely-replaced-by-automation-testing)__). \n",
    "\n",
    "#### Research Questions\n",
    "More specifically, this project aims to answer the following questions:\n",
    "\n",
    "* What are the most popular skills required for prospective software testers?\n",
    "* Which programming languages are the most popular for test automation?\n",
    "\n",
    "#### Future Work / Project Limitations \n",
    "\n",
    "Some questions that are interesting, but are out of scope for this project are the the ones below:\n",
    "\n",
    "* How many job adverts mention a combination of manual and test automation both in their description?\n",
    "* How many job descriptions mention training / upskilling?\n",
    "* How many jobs exclusively focus on either manual or automation?\n",
    "\n",
    "The reason why these problems are out of scope is because the data set used in this project is limited in terms of geographical region. Additionally, for the questions listed above a much larger dataset would be required in order to come up with a sensible analysis. Indeed -- even with a larger dataset -- the work would still be limited to one language, and this excludes analysis from countries such as China or South Korea.\n",
    "\n",
    "Of course, one can limit the project to a specific language or region, but this kind of project still requires a large dataset that takes into consideration where the jobs are located (rural areas may have less to offer than urban ones), what time period they were posted in as well, what seniority level they address and which area of testing they are focused (e.g. performance testing). The deeper one dives into the topic, the more likely one is to find irregularities and different demands, which would make it difficult to generalise-- at least without performing some thorough analysis using metrics that are performed over a specific length of time. \n",
    "\n",
    "Also, the time required to answer such questions would be important as well: technology is in a constant state of flux, and it would be important to analyse job boards throughout a given period, with the focus placed in which areas change is happening the most and keeping an eye on how quickly some trends are growing.\n",
    "\n",
    "#### Motivation\n",
    "The motivation behind this specific area of research stems from my job as a QA where I have worked with both automation and manual testing tools. While I personally believe that a good software tester has skills in __[both white box and black box testing](https://www.geeksforgeeks.org/differences-between-black-box-testing-vs-white-box-testing/)__, I am keen to find out what philosophy the job board is leaning towards with data from job boards serving as a good basis for me to embrace this topic on a practical level. \n",
    "\n",
    "Moreover, I used to work for a software house that focused on gathering data for recruitment companies. During that time, I used to dabble a bit in job board searches in order to garner what the expectations / trends for software testers were. In fact, some of the observations related to the limits of the project come from direct exposure related to analysing data and working in relation to gathering such data.  \n",
    "\n",
    "#### Previous Exploration of the Topic \n",
    "While __[many articles ](https://www.infoq.com/test-automation/articles/)__ have been and continue to be published on the subject already, I have not seen any students discussing this topic in our university slack channel or express much interest in software testing otherwise. Of interest; however, are the __[annual surveys carried out by Practitest](https://www.practitest.com/qa-learningcenter/webinars/learn-from-2021-state-of-testing/)__ which analyses software testing trends and provides information on its current domain. It showcases the versatility and importance of testing, especially with the rise of AI and larger teams recognising the constant need for quality assurance. \n",
    "\n",
    "Outside of the internet, there are __[also many workshops and courses available on automation testing](https://www.theknowledgeacademy.com/courses/automation-and-penetration-testing/fundamentals-of-test-automation-/bristol/)__ with their main focus being upskilling of manual testers, which proves how important automation is as a skill set for budding software testers.  \n",
    "\n",
    "\n",
    "#### Acquisition of the Data Set and Choice of Data Source\n",
    "The dataset gathered comes from job boards that have public APIs available for web crawling purposes. In particular, the site \n",
    "__[The Programmable Web](https://www.programmableweb.com/news/top-10-jobs-apis-2021/brief/2021/06/30)__ proved to be a valuable source for finding APIs that were free and did not require any prior set up. \n",
    "\n",
    "Due to the fact that Indeed is the biggest job site aggregator, I decided to use it for my project, even though I would have preferred using a smaller site that provided guidance on how to use its API. However, I wanted to ensure that I would have sufficient data to do some preliminary analysis, which smaller job site aggregators would not necessarily have been able to offer. \n",
    "\n",
    "#### Ethical Implications \n",
    "\n",
    "Scraping data as _[the article ](https://www.empiricaldata.org/dataladyblog/a-guide-to-ethical-web-scraping)_ to ethical web scraping indicates can have ethical implications when dealing with sensitive data -- such as someone’s health records. These implications are important to consider, especially because malicious agents could use such sensitive information for criminal purposes -- such as a fraud. Therefore, it is important to respect the API provided by asking for permission and not aggressively scraping their site for data. \n",
    "\n",
    "While I have not asked Indeed for explicit permission, I have taken the ethics into consideration in the following manner, arguing that my use is ethical since Indeed is an aggregator that collects job listings from other sites -- this information is public and intended to be shared with other job seekers and recruiters alike. There is no sensitive information in the job listing, other than the job being offered, the requirements being sought and some general information about the company itself. \n",
    "\n",
    "While the data could be used to create a competitive aggregator site, the data required for such a task would have to be massive and is outside of this project’s scope, both in terms of experience and budget. Additionally, it should be noted that I am not interested in using the data for commercial purposes, but to satisfy the research questions outlined in this report before. \n",
    "\n",
    "As such, while the data shall be easily available within this Notebook, there is no objective to use the data beyonds its purpose of analysing the job market trends for software testers in the UK area, and the focus here specifically lies on remote job listings. As such, while the data shall be easily available within this Notebook, there is no objective to use the data beyonds its purpose of analysing the job market trends for software testers in the UK area, and the focus here specifically lies on remote job listings. As for the frequency of the scraping, I only intend to scrape the website once or twice to receive the data I wish to have, without the need to overload its services. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d871e0",
   "metadata": {},
   "source": [
    "### Initial Scraping of Data \n",
    "\n",
    "Before setting out to do a more thorough scraping of the data, I wanted to play around with the basics of it, which is why I wrote two scripts that dealt with web crawling  -- scraperHelpers.py and main.py. The former contains functions which help to scrape data from a given url, while the latter calls upon these methods in order to produce a txt file with the job titles listed on the first page.\n",
    "\n",
    "The original scripts will be provided as part of this midterm submission.\n",
    "\n",
    "```python\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from typing import List\n",
    "\n",
    "```\n",
    "\n",
    "The script uses BeautifulSoup in order to parse scraped data into a parsed HTML file, which can then be further analysed and processed. Also, the request library is utilised in order to get data from a given url. Both libraries  were chosen because of their ease of implementation and use. \n",
    "\n",
    "```python\n",
    "\n",
    "def retrieve_data_as_text(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Retrieves data from a url\n",
    "    :param url:\n",
    "    :return: data in form of a text\n",
    "    \"\"\"\n",
    "    return requests.get(url, HEADERS).text\n",
    "\n",
    "```\n",
    "The method below pulls out the text from the job advert which can, oftentimes, be more readable than getting a parsed HTML text -- this is good for testing purposes where the print() method can be called upon in order to verify that it is the correct data. This method is also reused in the parse_data_into_html_ method which is called in the main.py script.  \n",
    "\n",
    "```python\n",
    "\n",
    "def parse_data_into_html(url: str) -> BeautifulSoup:\n",
    "    \"\"\"\n",
    "    Retrieves data in html format\n",
    "    :param url: the url in string format\n",
    "    :return: data parsed into HTML\n",
    "    \"\"\"\n",
    "    data = retrieve_data_as_text(url)\n",
    "    soup = BeautifulSoup(data, 'html.parser')\n",
    "    return soup\n",
    "```\n",
    "\n",
    "The method below retrieves the URL as text and then using the BeautifulSoup object parses it into a HTML file. This file serves as the basis for all further processing and analysis. \n",
    "\n",
    "```python\n",
    "\n",
    "def find_jobs_by_header_title(scraped_data: BeautifulSoup) -> List:\n",
    "    \"\"\"\n",
    "    Finds jobs by header title\n",
    "    :param scraped_data:\n",
    "    :param scraped_data: the data to be retrieved\n",
    "    :return: a list of job titles\n",
    "    \"\"\"\n",
    "    jobs = []\n",
    "    # code credit for text splitting:\n",
    "    # @ https://www.geeksforgeeks.org/scraping-indeed-job-data-using-python\n",
    "    for item in scraped_data.find_all(\"h2\", class_=\"jobTitle\"):\n",
    "        data_str = \"\" + item.get_text()\n",
    "        jobs.append(data_str.split(\"\\n\"))\n",
    "    return jobs\n",
    "    \n",
    "```\n",
    "The method above  lists all the data that fall under the category of job title and appends them to a list \n",
    "of job titles, which is then reused in the code below in order to create a text file with all the roles found in one page. \n",
    "\n",
    "```python\n",
    "\n",
    "def save_jobs_as_txt(jobs: List):\n",
    "    \"\"\"\n",
    "    Saves job titles into a text file\n",
    "    :param jobs: a list of jobs\n",
    "    :return: returns a txt file containing job titles\n",
    "    \"\"\"\n",
    "    with open('job_titles.txt', 'w') as f:\n",
    "        f.write(\"\\n\".join(str(job) for job in jobs))\n",
    "\n",
    "```\n",
    "\n",
    "The code above is called from a script called main that processes the data above into a text file containing all the available job titles. Separating the web scraping logic from the main file is a way to make the code reusable. \n",
    "\n",
    "```python\n",
    "\n",
    "# !/usr/bin/python\n",
    "# !python\n",
    "from scraperHelpers import *\n",
    "\n",
    "URL = 'https://uk.indeed.com/Remote-QA-jobs'\n",
    "\n",
    "scraped_data = parse_data_into_html(URL)\n",
    "jobs = find_jobs_by_header_title(scraped_data)\n",
    "save_jobs_as_txt(jobs)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1142253",
   "metadata": {},
   "source": [
    "### Scraping and Saving Data into a CSV File : The ScraperHelpers Script\n",
    "\n",
    "As with the previous code example, these code samples were taken from the script written in PyCharm -- this was done in order to save time and focus on having the data ready to play with for future analysis. The two functions below are an extension to the ones depicted above. As mentioned before, the original scripts will be provided as part of this midterm submission. \n",
    "\n",
    "```python\n",
    "\n",
    "#!/usr/bin/python\n",
    "# !python\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "from typing import List\n",
    "\n",
    "# global variables\n",
    "PAGE_COUNT_ITR = 250\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:88.0) Gecko/20100101 Firefox/88.0\"}\n",
    "\n",
    "def retrieve_pages_as_text() -> List:\n",
    "    \"\"\"\n",
    "    Scrapes pages in the form of text\n",
    "    :return: Returns a list of retrieved pages in the form of text\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    page_count = 0\n",
    "    while page_count <= PAGE_COUNT_ITR:\n",
    "        url = f'https://uk.indeed.com/jobs?q=Remote%20QA&sort=date&start={page_count}'\n",
    "        data_list.append(retrieve_data_as_text(url))\n",
    "        page_count += 10\n",
    "    return data_list\n",
    "```\n",
    "\n",
    "The method above returns a list of page texts taken from a given number of pages on the LinkedIn page. A while loop is used to increment the page count until a specific limit has been reached; the page count itself is added as a parameter to the URL.\n",
    "\n",
    "```python\n",
    "def extract_job_descriptions() -> List:\n",
    "    \"\"\"\n",
    "    @Credit for this piece of code goes to:\n",
    "    https://stackoverflow.com/questions/67504953/\n",
    "    how-to-get-full-job-descriptions-from-indeed-using-python-and-beautifulsoup\n",
    "    Any modifications are mine and mine alone\n",
    "\n",
    "    Retrieve job summaries  from multiple pages\n",
    "    :return: a list of job descriptions\n",
    "    \"\"\"\n",
    "    job_summaries = []\n",
    "    api_url = \"https://uk.indeed.com/viewjob?viewtype=embedded&jk={job_id}\"\n",
    "    url = \"https://uk.indeed.com/jobs?q=Remote%20QA\"\n",
    "    scraped_data = BeautifulSoup(requests.get(url, headers=HEADERS).content, \"html.parser\")\n",
    "\n",
    "    for job in scraped_data.select('a[id^=\"job_\"]'):\n",
    "        job_id = job[\"id\"].split(\"_\")[-1]\n",
    "        scraped_job_data = BeautifulSoup(requests.get(api_url.format(job_id=job_id),\n",
    "                                                      headers=HEADERS).content, \"html.parser\")\n",
    "        job_description = scraped_job_data.select_one(\"#jobDescriptionText\").get_text(strip=True)\n",
    "        job_summaries.append(job_description)\n",
    "\n",
    "    return job_summaries\n",
    "```   \n",
    "The method above returns a list of job summaries extracted from a single page on Indeed. Headers are used in order to retrieve\n",
    "more than one job id. \n",
    "\n",
    "```python\n",
    "def parse_page_data_into_html() -> List:\n",
    "    \"\"\"\n",
    "    Retrieves page data in html format\n",
    "    :return: parsed page data as HTML supplied into a list\n",
    "    \"\"\"\n",
    "    data_list = retrieve_pages_as_text()\n",
    "    soup = []\n",
    "    for data in data_list:\n",
    "        soup.append(BeautifulSoup(data, 'html.parser'))\n",
    "    return soup\n",
    "```   \n",
    "The method above returns a list of parsed HTML data which is further used to extract job titles from a select number of pages.\n",
    "\n",
    "```python\n",
    "\n",
    "# noinspection PyTypeChecker\n",
    "def save_summaries_as_csv(extracted_job_titles: List):\n",
    "    \"\"\"\n",
    "    Save the job summaries into a csv file\n",
    "    :param extracted_job_titles:\n",
    "    :return: a csv file containing job summaries\n",
    "    \"\"\"\n",
    "    data_frame = pd.DataFrame(extracted_job_titles)\n",
    "    data_frame.to_csv(\"job_descriptions.csv\")\n",
    "\n",
    "\n",
    "# noinspection PyTypeChecker\n",
    "def save_titles_as_csv(extracted_job_titles: List):\n",
    "    \"\"\"\n",
    "    Saves the extracted job titles into a csv file\n",
    "    :param extracted_job_titles:\n",
    "    :return: a csv file containing job titles\n",
    "    \"\"\"\n",
    "    data_frame = pd.DataFrame(extracted_job_titles)\n",
    "    data_frame.to_csv(\"job_titles.csv\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ae0e6c",
   "metadata": {},
   "source": [
    "#### The Requirements and Main File \n",
    "\n",
    "The requirements.txt which will be supplied along with the original scripts contains the following libaries:\n",
    "    \n",
    "beautifulsoup4==4.10.0\n",
    "\n",
    "bs4==0.0.1\n",
    "\n",
    "certifi==2021.10.8\n",
    "\n",
    "charset-normalizer==2.0.7\n",
    "\n",
    "idna==3.3\n",
    "\n",
    "numpy==1.21.4\n",
    "\n",
    "pandas==1.3.4\n",
    "\n",
    "python-dateutil==2.8.2\n",
    "\n",
    "pytz==2021.3\n",
    "\n",
    "requests==2.26.0\n",
    "\n",
    "six==1.16.0\n",
    "\n",
    "soupsieve==2.3\n",
    "\n",
    "urllib3==1.26.7\n",
    "\n",
    "The requirements.txt was generated in PyCharm using the pip freeze > requirements.txt command. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b708702",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7a3b23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
